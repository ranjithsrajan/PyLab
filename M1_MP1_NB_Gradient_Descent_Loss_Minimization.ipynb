{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranjithsrajan/PyLab/blob/main/M1_MP1_NB_Gradient_Descent_Loss_Minimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "powered-thong"
      },
      "source": [
        "# Applied Data Science and Machine Learning\n",
        "## A program by IIT Madras and TalentSprint\n",
        "### Mini Project: Gradient Descent Loss Minimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nK0fzdQzk0g"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the mini project, you will be able to\n",
        "\n",
        "* understand Optimization, apply optimization algorithms\n",
        "* Understand the intuition behind ordinay least squares (OLS). How is the best fit found? How do you actually implement gradient descent?\n",
        "* Loading and looking at data and implementing\n",
        "  - Cost function\n",
        "  - Gradient descent variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hungry-accident"
      },
      "source": [
        "**Packages used:**  \n",
        "* `Pandas` for data frames and easy to read csv files  \n",
        "* `Numpy` for array and matrix mathematics functions  \n",
        "* `Matplotlib` and `Seaborn` for visualization\n",
        "*  `sklearn` for the metrics and pre-processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AClW5O_SI7cL"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "The dataset chosen for this mini project is [California Housing Price Data Set](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html) with 20640 instances each having 9 attributes.\n",
        "\n",
        "The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. The data aren't cleaned so there are some preprocessing steps required! The columns are as follows, their names are self explanatory:\n",
        "\n",
        ":Attribute Information:\n",
        "\n",
        "    - MedInc        median income in block group\n",
        "    - HouseAge      median house age in block group\n",
        "    - AveRooms      average number of rooms per household\n",
        "    - AveBedrms     average number of bedrooms per household\n",
        "    - Population    block group population\n",
        "    - AveOccup      average number of household members\n",
        "    - Latitude      block group latitude\n",
        "    - Longitude     block group longitude\n",
        "    - Housing_Value   the median of the house value for California district\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndQNKsjS7c04"
      },
      "source": [
        "## Grading = 10 Points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_o83LguMwtQ"
      },
      "source": [
        "### Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX6BvXCkf5rK",
        "cellView": "form"
      },
      "source": [
        "#@title Load the dataset\n",
        "!wget -qq https://cdn.extras.talentsprint.com/ADSMI/Datasets/California_housing.csv"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxyTZ_I_zk0l"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXohOhDgzk0m"
      },
      "source": [
        "**Exercise 1**: Load the data and perform the following (2 points)\n",
        "- Exploratory Data Analysis\n",
        "- Preprocessing\n",
        "\n",
        "\n",
        "**Hints:**\n",
        "- Import the dataset as dataframe using `read_csv()` method of `pandas` package\n",
        "- checking for the number of rows and columns using `shape` attribute of `pandas` dataframe\n",
        "- Show the top 5 and the last 5 rows of the data using `head()` method\n",
        "- summary of the dataset\n",
        "- statistical description of the features using `describe()` method of `pandas` package\n",
        "- check for the null values, and handle them if *any*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwcGzaeBzk0n"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TREXlFBuYojw"
      },
      "source": [
        "**Exercise 2**: Data Visualization (2 points)\n",
        "\n",
        "- Visualize the Latitude and Longitude using the Population as size using `plot()` method and experiment with various parameters of that method\n",
        "- Plot the distribution of all the variables as histograms using `hist()` method of dataframe object and set the `bins` parameter\n",
        "- Correlations between variables as heatmap using `heatmap()` method of `seaborn` package\n",
        "- Analyze the results between target and other features using `pairplot()` function of `seaborn` package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1CDuBsmZmof"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcgRBeAmU7B2"
      },
      "source": [
        "**Exercise 3:** Data Preparation (3 Points)\n",
        "\n",
        " - Create new features:\n",
        "\n",
        "    - Create possible new features based on the existing data\n",
        "  \n",
        "      For eg. `bedrooms per room` = AveBedrms / AveRooms\n",
        "\n",
        "- Identify the `features`, `target` from the given set of attributes\n",
        "\n",
        "  - Save the `features` in **features** variable and `target` column (our target column for prediction) in **target** variable\n",
        "  - Use `drop()` method of dataframe to drop target column when saving remaining columns to `features` variable\n",
        "  \n",
        "  (ex. `features = df.drop('target_column', axis = 1)`\n",
        "\n",
        "\n",
        "\n",
        "- Split the data (**features**, **target**) into train, test using `train_test_split()` method of `sklearn.model_selection()` package\n",
        "\n",
        "- Normalization\n",
        "  - Normalize the numerical columns of dataset using `StandardScaler()` or `MinMaxScaler()` and fit them to the dataset using `fit()` method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yajmSaP9p0Je"
      },
      "source": [
        "#  YOUR CODE HERE"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kboWB_DRDGj"
      },
      "source": [
        "## Find the best fit line using\n",
        "\n",
        "- Gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zct12J8FLLoQ"
      },
      "source": [
        "### Gradient Descent loss minimization\n",
        "\n",
        "\n",
        "The idea behind gradient descent is by gradually tuning parameters, such as slope (m) and the intercept (b) in our regression function $y = mx + b$, Minimize cost of a function that tells how far off model predicted result. For regression problems use mean squared error (MSE) cost function.\n",
        "\n",
        "$$ MSE = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 \\quad \\textrm{where} \\quad \\hat{y_i} = mx_i + b $$\n",
        "Now, Figure out how to tweak parameters m and b to reduce MSE.\n",
        "\n",
        "**Partial Derivatives**\n",
        "\n",
        "Use partial derivatives to find how each individual parameter affects MSE. Take the derivative with respect to m and b separately. Take a look at the formula below. It looks almost exactly the same as MSE, but this time add f(m, b) to it. then plug m and b numbers into it and calculate the result.\n",
        "\n",
        "$$ùëì(ùëö,ùëè)= \\frac{1}{n}\\sum_{i=1}^{n}(y_i - (mx_i+b))^2$$\n",
        "This function is better representation for further calculations of partial derivatives.\n",
        "\n",
        "**Partial Derivative With Respect to m**\n",
        "\n",
        "With respect to m means we derive parameter m and ignore what is going on with b, or it is 0. To derive with respect to m we will use chain rule.\n",
        "\n",
        "$$ [f(g(x))]' = f'(g(x)) * g(x)' \\: - \\textrm{chain rule}$$\n",
        "Chain rule applies when one function sits inside of another. Lets write these steps down:\n",
        "\n",
        "$$ (y - (mx + b))^2 $$\n",
        "1. Derivative of $(f)^2$ is $2(f)$, same as $x^2$ becomes $2x$\n",
        "2. We do nothing with $y - (mx + b)$, so it stays the same\n",
        "3. Derivative of $y - (mx + b)$ with respect to m is $(0 - (x + 0))$ or $-x$, because y and b are constants, they become 0, and derivative of mx is x\n",
        "\n",
        "Multiply all parts we get following: $2 * (y - (mx+b)) * -x$. if we move -x to the left: $-2x *(y-(mx+b))$. The final version of our derivative is the following:\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial m} = \\frac{1}{n}\\sum_{i=1}^{n}-2x_i(y_i - (mx_i+b))$$\n",
        "Here, $\\frac{df}{dm}$ means we find partial derivative of function f (we mentioned it earlier) with respect to m.\n",
        "\n",
        "**Partial Derivative With Respect to b**\n",
        "\n",
        "Same rules apply to the derivative with respect to b.\n",
        "\n",
        "  $y - (mx + b)$ becomes $(0 - (0 + 1))$ or $-1$, because y and mx are constants, they become 0, and derivative of b is 1\n",
        "\n",
        "Multiply all the parts together and we get $-2(y-(mx+b))$\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^{n}-2(y_i - (mx_i+b))$$\n",
        "\n",
        "**Final Function**\n",
        "\n",
        "1. Gradient descent is an iterative process and with each iteration (epoch) we slightly minimizing MSE, so each time we use our derived functions to update parameters m and b\n",
        "2. Because its iterative, we should choose how many iterations we take, or make algorithm stop when we approach minima of MSE. In other words when algorithm is no longer improving MSE, we know it reached minimum.\n",
        "3. Gradient descent has an additional parameter learning rate (lr), which helps control how fast or slow algorithm going towards minima of MSE\n",
        "Thats about it. So you can already understand that Gradient Descent for the most part is just process of taking derivatives and using them over and over to minimize function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fGG4zfLbwBH"
      },
      "source": [
        "**Exercise 4:** Find the best fit line using Gradient descent and visualize the results (3 points)\n",
        "\n",
        "**Hints:**\n",
        "       \n",
        "- write a function to return predicted(y), when inputs are : x, weight and intercept\n",
        "- write a function to return linear loss, when inputs are : y_true and y_predicted\n",
        "- write a function to return derivative of loss w.r.t weight, when inputs are : x, y_true, and y_predicted\n",
        "- write a function to return derivative of loss w.r.t bias, when inputs are : y_true, and y_predicted\n",
        "- write a function to perform gradient descent, when inputs are : x and y\n",
        "- write a function to return derivative of loss w.r.t bias, when inputs are : y_true, and y_predicted\n",
        "- use all the above functions to predict, display the results and the accuracy\n",
        "\n",
        "Read this [article](https://medium.com/analytics-vidhya/implementing-gradient-descent-for-multi-linear-regression-from-scratch-3e31c114ae12) for more information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtO1KpcIdoP-"
      },
      "source": [
        "#  YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwVX18lC_738"
      },
      "source": [
        "# YOUR CODE(s) HERE"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}