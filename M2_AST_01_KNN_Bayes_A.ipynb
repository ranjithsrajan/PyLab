{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hungry-accident"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranjithsrajan/PyLab/blob/main/M2_AST_01_KNN_Bayes_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNKUyhSsoKCO"
      },
      "source": [
        "# Applied Data Science and Machine Learning\n",
        "## A program by IIT Madras and TalentSprint\n",
        "### Assignment 1:  KNN and Bayes Rule"
      ],
      "id": "mNKUyhSsoKCO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hungry-accident"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* understand k-Nearest Neighbors and implement from scratch\n",
        "* implement k-Nearest Neighbors and verifying using sklearn library\n",
        "* understand terms related to Bayesian Inference"
      ],
      "id": "hungry-accident"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adaptive-ghost"
      },
      "source": [
        "## Information"
      ],
      "id": "adaptive-ghost"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### k-Nearest Neighbors\n",
        "\n",
        "The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other. The KNN captures the idea of similarity (sometimes called distance, proximity, or closeness). Usually the Eucledian distance is used as the distance for the nearness measure.\n",
        "\n",
        "K nearest neighbors is a supervised machine learning algorithm often used in classification problems.  It works on the simple assumption that “The apple does not fall far from the tree” meaning similar things are always in close proximity. This algorithm works by classifying the data points based on how the neighbors are classified. Any new case is classified based on a similarity measure of all the available cases.\n",
        "\n",
        "**Features of KNN –**\n",
        "\n",
        "* Lazy Learning Algorithm - It is a lazy learner because it does not have a training phase but rather memorizes the training dataset. All computations are delayed until classification.\n",
        "\n",
        "* Case-Based Learning Algorithm -The algorithm uses raw training instances from the problem domain to make predictions and is often referred to as an instance based or case-based learning algorithm. Case-based learning implies that KNN does not explicitly learn a model. Rather it memorizes the training instances/cases which are then used as “knowledge” for the prediction phase. Given an input, when we ask the algorithm to predict a label, it will make use of the memorized training instances to give out an answer.\n",
        "\n",
        "* Non-Parametric — A non-parametric method has either a fixed number of parameters regardless of the data size or has no parameters. In KNN, irrespective of the size of data, the only unknown parameter is K. There are no assumptions made about the functional form of the problem is solved but here is a trade-off as this comes with a computation cost. An important point to note is that KNN has minimal training phase but this comes both at a memory cost and as well computational cost. Memory cost because it requires storing a huge data set and computation cost during the test time because classifying a given observation needs a run-down of the complete data set.\n",
        "\n",
        "[K-Nearest Neighbors Demo](http://vision.stanford.edu/teaching/cs231n-demos/knn/)"
      ],
      "metadata": {
        "id": "W6tRkkqIo9ip"
      },
      "id": "W6tRkkqIo9ip"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbYiJYYMaU02"
      },
      "source": [
        "**BAYSIAN INFERENCE** : Why we need Bayesian inference?\n",
        "\n",
        "In general, statistical inference is the process of determining properties of a model/distribution, given some data. Bayesian inference can be seen as the Bayesian counterpart to frequentist inference. In Frequentist inference, there is usually the notion of some true, unknown, parameter which is a constant, and point estimates are inferred from data. Contrarily, Bayesian inference treats the model parameters as random variables and usually wants to deduce probabilistic statements about the distribution of parameters.\n",
        "\n",
        "Terminology\n",
        "\n",
        "The basic terms related to Bayesian inference are as follows:\n",
        "\n",
        "Prior: the probability distribution that would express one's beliefs about an uncertain quantity before some evidence is taken into account.\n",
        "Posterior: in Bayesian statistics, it is the revised or updated probability of an event occurring after taking into consideration new information."
      ],
      "id": "NbYiJYYMaU02"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty_d-VQqqUQ1"
      },
      "source": [
        "### Setup Steps:"
      ],
      "id": "Ty_d-VQqqUQ1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDMkEqfJqUQ3"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "id": "MDMkEqfJqUQ3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9I3Sk8PqUQ4"
      },
      "outputs": [],
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "id": "v9I3Sk8PqUQ4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "toJ3sOFqqUQ4"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M2_AST_01_KNN_Bayes_A\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://learn-iitm.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "id": "toJ3sOFqqUQ4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cdsx3o4LoL6"
      },
      "source": [
        "### Import required packages"
      ],
      "id": "3cdsx3o4LoL6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFZsmzadarQL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import norm, beta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Counter function to count the number of points\n",
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "AFZsmzadarQL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGvo3Ih9chNp"
      },
      "source": [
        "### K-Nearest Neighbors\n",
        "\n",
        "K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.\n",
        "K-NN is a non-parametric algorithm, which means it does not make any assumption on underlying data. It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.\n",
        "\n",
        "\n",
        "\n",
        "**How does K-NN work?**\n",
        "The K-NN working can be explained on the basis of the below algorithm:\n",
        "\n",
        "\n",
        "Step-1: Select the number K of the neighbors\n",
        "\n",
        "Step-2: Calculate the Euclidean distance of K number of neighbors\n",
        "\n",
        "Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.\n",
        "\n",
        "Step-4: Among these k neighbors, count the number of the data points in each category.\n",
        "\n",
        "Step-5: Assign the new data points to that category for which the number of the neighbor is maximum.\n",
        "\n",
        "Step-6: The Model is generated.\n",
        "\n",
        "![](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning3.png)\n",
        "\n",
        "![](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning4.png)\n",
        "\n",
        "![](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning5.png)"
      ],
      "id": "XGvo3Ih9chNp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qej1r6rqchKc"
      },
      "source": [
        "#### How to choose the value for K?\n",
        "\n",
        "K is a crucial parameter in the KNN algorithm. Some suggestions for choosing K Value are:\n",
        "\n",
        "1. Using error curves: The figure below shows error curves for different values of K for training and test data.\n",
        "\n",
        "2. K value should be odd while considering binary(two-class) classification."
      ],
      "id": "Qej1r6rqchKc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfdGSav5eFL_"
      },
      "source": [
        "#### Data Preparation\n",
        "\n",
        "After loading important libraries, we create our data using sklearn.datasets with 200 samples, 8 features, and 2 classes. Then data is split into the train(80%) and test(20%) data and scaled using StandardScaler."
      ],
      "id": "EfdGSav5eFL_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFY-XX14eClP"
      },
      "source": [
        "# Generate classification data\n",
        "X,Y = make_classification(n_samples= 200,n_features=8,\n",
        "                          n_informative=8,n_redundant=0,\n",
        "                          n_repeated=0,n_classes=2,\n",
        "                          random_state=14)\n",
        "\n",
        "# split the data into train and test\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "jFY-XX14eClP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalization\n",
        "\n",
        "Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information. Normalization is also required for some algorithms to model the data correctly.\n",
        "\n",
        "For more information please visit [Feature Scaling](https://medium.com/analytics-vidhya/feature-scaling-normalization-standardization-and-scaling-c920ed3637e7)\n"
      ],
      "metadata": {
        "id": "h-Ke4cvHpMH5"
      },
      "id": "h-Ke4cvHpMH5"
    },
    {
      "cell_type": "code",
      "source": [
        "# apply normalization\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "TpDAb1wcMtTn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "TpDAb1wcMtTn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement KNN from scratch"
      ],
      "metadata": {
        "id": "gSPkkxRaZNP2"
      },
      "id": "gSPkkxRaZNP2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Distance measures\n",
        "\n",
        "Function to compute the Eucledian distance between 2 points a and b"
      ],
      "metadata": {
        "id": "sxVXiI3IU69t"
      },
      "id": "sxVXiI3IU69t"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate distance between two points\n",
        "\n",
        "def eucledian_distance(a, b):\n",
        "    # YOUR CODE HERE\n",
        "    return distance"
      ],
      "metadata": {
        "id": "JpLBg4yRUw1U"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JpLBg4yRUw1U"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a knn function which takes train, test and k as input to produce predictions. For every test data point, calculate the distance and assign the data point to respective group."
      ],
      "metadata": {
        "id": "47eIc76MrkDP"
      },
      "id": "47eIc76MrkDP"
    },
    {
      "cell_type": "code",
      "source": [
        "def knn_custom_function(X_train, X_test, y_train, y_test, k):\n",
        "\n",
        "    # Prediction outputs of the test data\n",
        "    y_pred_test = []\n",
        "\n",
        "    for test_point in X_test:\n",
        "        distances = []\n",
        "\n",
        "        for train_point in X_train:\n",
        "            # calculate euclidean distance of data points\n",
        "            # YOUR CODE HERE\n",
        "\n",
        "        # Store distances in a dataframe\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "\n",
        "        # Sort the data by increasing distances, and choose only the k nearest points\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Create counter object\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Get most common label of all the nearest neighbors\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Prediction outputs of the test data\n",
        "         y_pred_test.append(prediction)\n",
        "\n",
        "    return y_pred_test"
      ],
      "metadata": {
        "id": "s5LNP69aUwt5"
      },
      "execution_count": null,
      "outputs": [],
      "id": "s5LNP69aUwt5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict with the custom functions"
      ],
      "metadata": {
        "id": "iJ9ApH1qZZja"
      },
      "id": "iJ9ApH1qZZja"
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test dataset\n",
        "y_pred_test = knn_custom_function(X_train, X_test, y_train, y_test, k=5)"
      ],
      "metadata": {
        "id": "YH3E4l6mUwqY"
      },
      "execution_count": null,
      "outputs": [],
      "id": "YH3E4l6mUwqY"
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the accuracy of the classifier model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "kYxkKe9Gaj-w"
      },
      "execution_count": null,
      "outputs": [],
      "id": "kYxkKe9Gaj-w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN classifier using Sklearn"
      ],
      "metadata": {
        "id": "rHCCF0fErU1d"
      },
      "id": "rHCCF0fErU1d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmbSWdBGeK-r"
      },
      "source": [
        "#### Find the value for K\n",
        "\n",
        "\n",
        "For choosing the K value, we use error curves and K value with optimal variance, and bias error is chosen as K value for prediction purposes. With the error curve plotted below, we choose K=7 for the prediction"
      ],
      "id": "dmbSWdBGeK-r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize KNeighborsClassifier from sklearn with n_neighbors as a paramter and fit the train data to train the model.\n",
        "\n",
        "The trained model can be used for predicting the unknown data (test data)"
      ],
      "metadata": {
        "id": "Z8ZzvlsEpydS"
      },
      "id": "Z8ZzvlsEpydS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initalizing the KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=7)\n",
        "# fit the data\n",
        "knn.fit(X_train,y_train)\n",
        "# predicting the test data\n",
        "y_pred1 = knn.predict(X_train)\n",
        "print(y_pred1)"
      ],
      "metadata": {
        "id": "jOGlTDwmpvKX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "jOGlTDwmpvKX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To pick the best k-value, iterate over different k-values with the KNN classifier and plot the error. Based on the least error select the best possible k-value."
      ],
      "metadata": {
        "id": "Cs8fAv4OqVyG"
      },
      "id": "Cs8fAv4OqVyG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz6ccA3XeCiO"
      },
      "source": [
        "error1 = []\n",
        "error2 = []\n",
        "for k in range(1,15):\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "# plt.figure(figsize(10,5))\n",
        "plt.plot(range(1,15),error1,label=\"train\")\n",
        "plt.plot(range(1,15),error2,label=\"test\")\n",
        "plt.xlabel('k Value')\n",
        "plt.ylabel('Error')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "dz6ccA3XeCiO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX7h3e7GeS2s"
      },
      "source": [
        "#### Implementing KNN from sklearn and Predict the data\n",
        "\n",
        "we have chosen the K value to be 5. Now we substitute that value and get the accuracy score for the test data.\n",
        "\n",
        "For more information, please refer to the documentation: [knn-sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
      ],
      "id": "AX7h3e7GeS2s"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szAqch61eCfe"
      },
      "source": [
        "# create the classifier\n",
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "szAqch61eCfe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the training data\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "fUlVUexKQjbM"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fUlVUexKQjbM"
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the test data\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "B8ACEmv_QjWX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "B8ACEmv_QjWX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classification Accuracy\n",
        "\n",
        "Classification Accuracy is the ratio of number of correct predictions to the total number of input samples.\n",
        "\n",
        "$$Accuracy = \\frac{No.of \\space correct \\space predictions}{Total \\space No.of\\space  predictions \\space made}$$\n",
        "\n",
        "It works well if there are equal number of samples belonging to each class(balanced).\n",
        "\n",
        "For example, consider that there are 98% samples of class A and 2% samples of class B in our training set. Then our model can easily get 98% training accuracy by simply predicting every training sample belonging to class A.\n"
      ],
      "metadata": {
        "id": "8Kdhemx6sZTg"
      },
      "id": "8Kdhemx6sZTg"
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the accuracy of the classifier model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "fq-PXCn9QjSD"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fq-PXCn9QjSD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Try with different values of k and plot the loss vs k\n",
        "\n",
        "**Hint**: Use loss = 1- accuracy score"
      ],
      "metadata": {
        "id": "dqOPLc9lmwgc"
      },
      "id": "dqOPLc9lmwgc"
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE Ungraded Exercise: Try with different values of k and plot the loss vs k"
      ],
      "metadata": {
        "id": "gYB-D02Cmwgl"
      },
      "execution_count": null,
      "outputs": [],
      "id": "gYB-D02Cmwgl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm-36INaat-i"
      },
      "source": [
        "### Bayesian inference\n",
        "\n",
        "Bayesian inference utilizes the famous Bayes rule:\n",
        "\n",
        "$$ P(A|B) = \\frac{P(B|A).P(A)}{P(B)} $$\n",
        "\n",
        "For model based inference, we can replace  $A$  with the parameters  $θ$  and  $B$  with the data  $D$  at interest. Furthermore, we can introduce  $I$  which can be used to introduce an additional assumption (knowledge) to the inference such as which model to use.\n",
        "\n",
        "$$ \\overbrace{P(\\theta| D, I)}^{\\text{posterior}} = \\frac{\\overbrace{P(D | \\theta, I)}^{\\text{likelihood}}\\overbrace{P(\\theta|I)}^{\\text{prior}}}{\\underbrace{P(D|I)}_{\\text{marginal likelihood}}} $$\n",
        "\n",
        "The prior distribution  $P(θ|I)$ specifies our assumption about the parameters  $θ$  before taking the data into account. The likelihood  $P(D|θ,I)$  represents the probability of the data if the parameters  $θ$  are specified. The marginal likelihood (or evidence)  $P(D|I)$  is the distribution of the data  $D$  given our additional assumption  $I$ . It is the normalization of the Bayes rule and plays an important role in model comparison. Finally, the posterior  $P(θ|D,I)$  is the distribution of the parameters after taking the observed data  $D$  and our additional (prior) assumption  $I$  into account. We can also say that the posterior is proportional to the likelihood and the prior.\n",
        "\n",
        "$$ posterior ∝ likelihood × prior $$\n",
        "\n",
        "To understand Bayesian inference interactively, click [here](https://seeing-theory.brown.edu/bayesian-inference/index.html)."
      ],
      "id": "gm-36INaat-i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35W8J8Ava1az"
      },
      "source": [
        "#### **Exercise 1:**\n",
        "\n",
        "We believe the probability of getting heads in a coin toss to be some true value p, but have no prior opinion on what p is. So we flip a coin a few times and record what's observed for each flip. Let's see how do our beliefs change as we observe more data?"
      ],
      "id": "35W8J8Ava1az"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbPw2z8Ca5Jk"
      },
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "n_trials = [0, 1, 2, 3, 4, 5, 8, 15, 50, 100, 200, 500]    # specify different number of trials\n",
        "# store the outcome of 500 coin tosses\n",
        "data = ? # YOUR CODE HERE\n",
        "# possible values for probability\n",
        "x = ?  # YOUR CODE HERE\n",
        "\n",
        "for k, N in enumerate(n_trials):\n",
        "    sx = plt.subplot(int(len(n_trials)/2), 2, k+1)\n",
        "    plt.xlabel('$p$, probability of heads') if k in [0, len(n_trials)-1] else None\n",
        "    plt.setp(sx.get_yticklabels(), visible=False)\n",
        "    heads = data[:N].sum()                # get number of heads from N trials\n",
        "    a = 1                                 # alpha parameter for Beta distribution as prior\n",
        "    b = 1                                 # beta parameter for Beta distribution as prior\n",
        "    s = heads                             # number of successes observed in N trials\n",
        "    f = N-heads                           # number of failures observed in N trials\n",
        "    y = beta.pdf(x, a+s, b+f)             # posterior as Beta distribution with alpha= a+s, beta= b+f\n",
        "    # Visualize\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    leg = plt.legend()\n",
        "    leg.get_frame().set_alpha(0.4)\n",
        "    plt.autoscale(tight=True)\n",
        "\n",
        "plt.suptitle(\"Bayesian updating of posterior probabilities\",\n",
        "             y=1.02,\n",
        "             fontsize=14)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "id": "dbPw2z8Ca5Jk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQup0dDma3S8"
      },
      "source": [
        "From the above plots following observations can be made:\n",
        "\n",
        "belief under no information is that all possible heads probabilities are equally likely (as in the case of 0 tosses above)\n",
        "it may not be possible for a small number of observations to represent the true probability (as in the case of 3 tosses above)\n",
        "small sample sizes are incredibly sensitive to minor imbalances in outcome counts\n",
        "as the sample size grows, the pdf gets narrow and more probability is allocated to p=0.5"
      ],
      "id": "YQup0dDma3S8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m51wiPEp9T1"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ],
      "id": "4m51wiPEp9T1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVpko2ucp9UF"
      },
      "outputs": [],
      "source": [
        "# @title In KNN classification, what is the primary effect of choosing a very small value of K (e.g., K=1)?   { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"\" #@param [\"\", \"The decision boundary becomes very smooth and generalizes well to new data\", \"The classifier becomes more sensitive to noise and outliers in the training data.\"]"
      ],
      "id": "mVpko2ucp9UF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjAEjFGssRpe"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "id": "KjAEjFGssRpe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1P8866ssRpe"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "id": "B1P8866ssRpe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Tq8HTMpsRpe"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "id": "2Tq8HTMpsRpe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf-bFAj8sRpe"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "id": "tf-bFAj8sRpe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgHtrsx0sRpe"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "id": "SgHtrsx0sRpe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DTkh6ZUxsRpf"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "id": "DTkh6ZUxsRpf"
    }
  ]
}