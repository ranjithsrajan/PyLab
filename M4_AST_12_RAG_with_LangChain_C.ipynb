{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranjithsrajan/PyLab/blob/main/M4_AST_12_RAG_with_LangChain_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRk-o2aF7Gpj"
      },
      "source": [
        "# Applied Data Science and Machine Learning\n",
        "## A program by IITM and TalentSprint\n",
        "### Assignment 12: RAG - Retrieval Augmented Generation\n",
        "\n",
        "**(with OpenAI LLMs)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWr7GKpxpEwc"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "1. Load the Documents\n",
        "2. Splitting the documents into chunks\n",
        "3. Embedding the chunks and storing them in vector db\n",
        "4. Retrieving the relevant chunks to the query\n",
        " * Addressing Diversity\n",
        " * Addressing Specificity\n",
        "5. Connecting with LLM to get a final grounded answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc9b_a76b0RW"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWnst4BvQZ_r"
      },
      "source": [
        "> **RAG diagram:**\n",
        ">\n",
        "> <img src='https://drive.google.com/uc?id=1sCVvpsmtZEU1WSK1FFGMGHbEjrgtCNLi'>\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krKO3QbcQZ_z"
      },
      "source": [
        "> **Vector Store and Retrieval:**\n",
        ">\n",
        "> <img src='https://drive.google.com/uc?id=1_zX5gtSNrV8Qdx7Nz4_gMR8dCwvxCDS7' width=750px>\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AYo-OK9QZ_z"
      },
      "source": [
        "> **Embedding Model:**\n",
        ">\n",
        "> <img src='https://drive.google.com/uc?id=1HnvjGJ4HmpS-0wndpH-Q8cKMwIwWkTUe'>\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI62t_drQZ_0"
      },
      "source": [
        "> **Retrieval in Action:**\n",
        ">\n",
        "> <img src='https://drive.google.com/uc?id=1ry2TWFsewwqYP3Lw9muuPmbyuQqXwnYV' width=800px>\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfgDb8sWQZ_0"
      },
      "source": [
        "> **Example workflow with embedding model:**\n",
        ">\n",
        "><br>\n",
        ">\n",
        "> <img src='https://drive.google.com/uc?id=1zTuMMX54L2HrnmCYktTxVfMVrkIz8w15' width=600px>\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVc_4Z46sgGF"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "toJ3sOFqqUQ4",
        "outputId": "ea8e1f56-bb47-4d12-d4fd-0ac0e939af81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2419592&recordId=2420\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M4_AST_12_RAG_with_LangChain_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword(),\"batch\":\"IITM-PG-ADSML-07\"}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support,\"batch\":\"IITM-PG-ADSML-07\"}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://learn-iitm.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KsPoPtTZ8K5"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "v5_-MW5g3xVj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip -q install langchain==0.3.27\n",
        "!pip -q install openai==2.3.0\n",
        "!pip -q install langchain-core==0.3.79\n",
        "!pip -q install langchain-community==0.3.31\n",
        "!pip -q install sentence-transformers==5.1.1\n",
        "!pip -q install langchain-huggingface==0.3.1\n",
        "!pip -q install langchain-experimental==0.3.4\n",
        "!pip -q install langchainhub==0.1.21\n",
        "!pip -q install langchain-openai==0.3.35\n",
        "!pip -q install langchain-chroma==0.2.6\n",
        "!pip -q install chromadb==1.1.1\n",
        "!pip -q install pypdf==6.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayJlGGiXaBLN"
      },
      "source": [
        "### Import Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GU2BjXKhaD5W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import numpy as np\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XF7eEn4oaUJ"
      },
      "source": [
        "#### **Provide your OpenAI API key**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cgfRX83Jb5jQ"
      },
      "outputs": [],
      "source": [
        "# Read OpenAI key from Colab Secrets\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('OPENAI_API_KEY')           # <-- change this as per your Colab secret's name\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaLaiX0x-AKC"
      },
      "source": [
        "### Load LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "smjp5Nk--I5i"
      },
      "outputs": [],
      "source": [
        "# Load Model\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General query\n",
        "response = llm.invoke(\"What is the Capital of India?\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "20v1Qo56JNF1",
        "collapsed": true,
        "outputId": "c444e8b0-c05d-4f09-ceb4-ee094d8da864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of India is New Delhi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJBCqkTXCmg2"
      },
      "source": [
        "### **Loading the documents**\n",
        "\n",
        "[PDF Loader](https://docs.langchain.com/oss/javascript/integrations/document_loaders/file_loaders/pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e_GL3dr9GG05"
      },
      "outputs": [],
      "source": [
        "# UPLOAD the Docs first to this notebook, then run this cell\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Load PDF\n",
        "loaders = [\n",
        "    PyPDFLoader(\"/content/drive/MyDrive/pca_d1.pdf\"),\n",
        "    PyPDFLoader(\"/content/drive/MyDrive/ens_d2.pdf\"),\n",
        "    PyPDFLoader(\"/content/drive/MyDrive/ens_d2.pdf\"),    # Loading duplicate documents on purpose\n",
        "]\n",
        "\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    docs.extend(loader.load())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8mms8D126eTS",
        "outputId": "a64a2892-e9d1-47fa-a5f2-59a7859a7da2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Duzb1D3qPGix",
        "outputId": "61fcb9d2-ce26-4d4b-fbf2-7d8ef62b3918",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "len(docs)        # 7 pages were there in total from above documents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "-E4jRNJBzXBD",
        "outputId": "f9cc4243-59ae-4127-b8e2-1cce4acb01d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \n",
            " \n",
            " \n",
            "N \n",
            " \n",
            "1 Principal Component Analysis \n",
            "In real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the  \n",
            "data and find various patterns in it or use it to train some machine learning models.  One way to  \n",
            "think about dimensions is that suppose you have an data point x , if we consider this data point as \n",
            "a physical object then dimensions are merely a basis of view, like where is the data located when \n",
            "it is observed from horizontal axis or vertical axis. \n",
            "As the dimensions of data increases, the difficulty to visualize it and perform computations on \n",
            "it also increases. So, how to reduce the dimensions of a data:- \n",
            "• Remove the redundant dimensions \n",
            "• Only keep the most important dimensions  \n",
            "Let us first try to understand some terms:- \n",
            "Variance : It is a measure of the variability or it simply measures how spread the data set is.  \n",
            "Mathematically, it is the average squared deviation from the mean score. We use the following \n",
            "formula to compute variance var(x). \n",
            " \n",
            "var(x) = \n",
            "Σ(xi−x¯)2 \n",
            "N \n",
            "Covariance :  It is a measure of the extent to which corresponding elements from two sets of  \n",
            "ordered data move in the same direction. Formula is shown below denoted by cov(x,y) as the  \n",
            "covariance of x and y. \n",
            "var(x)  =  \n",
            "Σ(xi−x¯)(yi−y¯) \n",
            "• Here, xi is the value of x in ith dimension. \n",
            "• x bar and y bar denote the corresponding mean values. \n",
            "• One way to observe the covariance is how interrelated two data sets are. \n",
            "Positive covariance means X and Y are positively related i.e. as X increases Y also increases.  \n",
            "Negative covariance depicts the exact opposite relation.  However zero covariance means X and Y  \n",
            "are not related. \n",
            "Now lets think about the requirement of data analysis. \n",
            "Since we try to find the patterns among the data sets so we want the data to be spread out  \n",
            "across each dimension. Also, we want the dimensions to be independent. Such that if data has high \n",
            "covariance when represented in some n number of dimensions then we replace those dimensions  \n",
            "with linear combination of those n dimensions. Now that data will only be dependent on linear  \n",
            "combination of those related n dimensions. (related = have high covariance)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NxinwPVCOkcI",
        "outputId": "5a9fe669-32ae-4be2-ea0d-3f92b8ed92c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-04-04T07:17:15+05:30', 'author': 'Ramendra Kumar', 'moddate': '2024-04-04T07:17:15+05:30', 'source': '/content/drive/MyDrive/pca_d1.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='1 \\n \\n \\nN \\n \\n1 Principal Component Analysis \\nIn real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the  \\ndata and find various patterns in it or use it to train some machine learning models.  One way to  \\nthink about dimensions is that suppose you have an data point x , if we consider this data point as \\na physical object then dimensions are merely a basis of view, like where is the data located when \\nit is observed from horizontal axis or vertical axis. \\nAs the dimensions of data increases, the difficulty to visualize it and perform computations on \\nit also increases. So, how to reduce the dimensions of a data:- \\n• Remove the redundant dimensions \\n• Only keep the most important dimensions  \\nLet us first try to understand some terms:- \\nVariance : It is a measure of the variability or it simply measures how spread the data set is.  \\nMathematically, it is the average squared deviation from the mean score. We use the following \\nformula to compute variance var(x). \\n \\nvar(x) = \\nΣ(xi−x¯)2 \\nN \\nCovariance :  It is a measure of the extent to which corresponding elements from two sets of  \\nordered data move in the same direction. Formula is shown below denoted by cov(x,y) as the  \\ncovariance of x and y. \\nvar(x)  =  \\nΣ(xi−x¯)(yi−y¯) \\n• Here, xi is the value of x in ith dimension. \\n• x bar and y bar denote the corresponding mean values. \\n• One way to observe the covariance is how interrelated two data sets are. \\nPositive covariance means X and Y are positively related i.e. as X increases Y also increases.  \\nNegative covariance depicts the exact opposite relation.  However zero covariance means X and Y  \\nare not related. \\nNow lets think about the requirement of data analysis. \\nSince we try to find the patterns among the data sets so we want the data to be spread out  \\nacross each dimension. Also, we want the dimensions to be independent. Such that if data has high \\ncovariance when represented in some n number of dimensions then we replace those dimensions  \\nwith linear combination of those n dimensions. Now that data will only be dependent on linear  \\ncombination of those related n dimensions. (related = have high covariance)'),\n",
              " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-04-04T07:17:15+05:30', 'author': 'Ramendra Kumar', 'moddate': '2024-04-04T07:17:15+05:30', 'source': '/content/drive/MyDrive/pca_d1.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='2 \\n \\n \\n \\nSo, what does Principal Component Analysis (PCA) do? \\nPCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are  \\northogonal (and hence linearly independent) and ranked according to the variance of data along  \\nthem. It means more important principle axis occurs first. (more important = more variance/more  \\nspread out data) \\n \\nHow does PCA work? \\n• Calculate the covariance matrix X of data points. \\n• Calculate eigenvectors and corresponding eigenvalues. \\n• Sort the eigenvectors according to their eigenvalues in decreasing order. \\n• Choose first k eigenvectors and that will be the new k dimensions. \\n• Transform the original n dimensional data points into k dimensions. \\n \\nTo understand the detail working of PCA, we should have knowledge of eigen values and eigen  \\nvectors \\n \\nEigenvectors: The directions in which our data are dispersed. \\nEigenvalues: The relative importance of these different directions. \\n \\n[Covariance matrix].[Eigenvector] = [eigenvalue].[Eigenvector] \\nLets look into what a covariance matrix is? \\nA covariance matrix of some data set in 4 dimensions a,b,c,d. \\n□ Va Ca,b Ca,c Ca,d Ca,e \\n  \\n□ Ca,b Vb Cb,c Cb,d Cb,e \\n  \\n \\n \\nVa : variance along dimension a \\nCa,b : Covariance along dimension a and b \\nIf we have a matrix X of m*n dimension such that it holds n data points of m dimensions, then \\ncovariance matrix can be calculated as \\nC   =    1  (X − X¯ )(X − X¯ )T \\n \\nx n−1 \\nIt is important to note that the covariance matrix contains:- \\n• variance of dimensions as the main diagonal elements. \\n• covariance of dimensions as the off diagonal elements. \\nAlso, covariance matrix is symmetric (observe from the image above) \\nCa,c Cb,c Vc Cc,d Cc e \\nCa,d Cb,d Cc,d Vd Cd e \\nCa,e Cb,e Cc,e Cd,e Ve'),\n",
              " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-04-04T07:17:15+05:30', 'author': 'Ramendra Kumar', 'moddate': '2024-04-04T07:17:15+05:30', 'source': '/content/drive/MyDrive/pca_d1.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='3 \\n \\n \\n \\nAs, we discussed earlier we want the data to be spread out i.e. it should have high variance along  \\ndimensions. Also  we want to remove correlated dimensions i.e. covariance among the dimensions  \\nshould be zero (they should be linearly independent). \\nTherefore, our covariance matrix should have:- \\n• large numbers as the main diagonal elements. \\n• zero values as the off diagonal elements. \\nWe call it a diagonal matrix. So, we have to transform the original data points such that their  \\ncovariance is a diagonal matrix. \\nAlways normalize your data before doing PCA if we use data (features here) of different scales, we \\nget misleading components. We can also simply use correlation matrix instead of using covariance  \\nmatrix if features are of different scales. \\nThis defines the goal of PCA:- \\n1. Find linearly independent dimensions which can losslessly represent the data points. \\n2. Those newly found dimensions should allow us to predict/reconstruct the original dimensions.'),\n",
              " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-04-04T07:17:46+05:30', 'author': 'Abhinav', 'moddate': '2024-04-04T07:17:46+05:30', 'source': '/content/drive/MyDrive/ens_d2.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='1  \\n \\nEnsemble Methods \\nLet us consider a real world situation which uses Ensemble Methods, which is, when a user wants \\nto buy a new product. Many users who have already purchased that product will have given either  \\npositive or negative ratings. If in the group, many users have given positive ratings, then the \\ncombined rating will be positive. Instead of a single rating, the ratings of the group of users is \\nconsidered. The product is bought by the user when the combined ratings of the group is positive. \\nThe user gets a fairer idea about the product when all the ratings are combined. \\nHere, the combination of ratings is done so that the decision making process of the user is made  \\neasy. \\nEnsemble Methods refer to combining many different machine learning models in order to get a  \\nmore powerful prediction. \\nThus, ensemble methods increase the accuracy of the predictions. \\n \\nWhy use Ensemble Methods? \\nEnsemble Methods are used in order to: \\n• decrease variance (bagging) \\n• decrease bias (boosting) \\n• improve predictions (stacking) \\n \\nBagging \\nBagging actually refers to Bootstrap Aggregators. \\nBagging tests multiple models on the data by sampling and replacing data i.e it utilizes bootstrap - \\nping. In turn, this reduces the noise and variance by utilizing multiple samples. Each hypothesis \\nhas the same weight as all the others. Now, aggregating of the outputs of various models is done. \\n \\nBoosting \\nBoosting is an iterative technique which adjusts the weight of an observation based on the last  \\nclassification. If an observation was classified incorrectly, it tries to increase the weight of this  \\nobservation and vice versa. Boosting in general decreases the bias error and builds strong predictive  \\nmodels. \\n \\nVariance \\nVariance quantifies how the predictions made on same observation are different from each other. A  \\nhigh variance model will over -fit on your training population and perform badly on any observation  \\nbeyond training. Thus, we aim at low variance.'),\n",
              " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-04-04T07:17:46+05:30', 'author': 'Abhinav', 'moddate': '2024-04-04T07:17:46+05:30', 'source': '/content/drive/MyDrive/ens_d2.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='2  \\n \\nBias \\nBias error is useful to quantify how much on on average are the predicted values different from the  \\nactual value. A high  bias  error means we have a under -performing model.  Thus,  we aim at low  \\nbias. \\nA commonly used class of ensemble methods are forests of randomized trees. \\nIn random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e. \\na bootstrap sample) from the training set. In addition, instead of using all the features, a random  \\nsubset of features is selected, further randomizing the tree. \\nAs a result, the bias of the forest increases slightly, but due to the averaging of less correlated  \\ntrees, its variance decreases, resulting in an overall better model.'),\n",
              " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-04-04T07:17:46+05:30', 'author': 'Abhinav', 'moddate': '2024-04-04T07:17:46+05:30', 'source': '/content/drive/MyDrive/ens_d2.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='1  \\n \\nEnsemble Methods \\nLet us consider a real world situation which uses Ensemble Methods, which is, when a user wants \\nto buy a new product. Many users who have already purchased that product will have given either  \\npositive or negative ratings. If in the group, many users have given positive ratings, then the \\ncombined rating will be positive. Instead of a single rating, the ratings of the group of users is \\nconsidered. The product is bought by the user when the combined ratings of the group is positive. \\nThe user gets a fairer idea about the product when all the ratings are combined. \\nHere, the combination of ratings is done so that the decision making process of the user is made  \\neasy. \\nEnsemble Methods refer to combining many different machine learning models in order to get a  \\nmore powerful prediction. \\nThus, ensemble methods increase the accuracy of the predictions. \\n \\nWhy use Ensemble Methods? \\nEnsemble Methods are used in order to: \\n• decrease variance (bagging) \\n• decrease bias (boosting) \\n• improve predictions (stacking) \\n \\nBagging \\nBagging actually refers to Bootstrap Aggregators. \\nBagging tests multiple models on the data by sampling and replacing data i.e it utilizes bootstrap - \\nping. In turn, this reduces the noise and variance by utilizing multiple samples. Each hypothesis \\nhas the same weight as all the others. Now, aggregating of the outputs of various models is done. \\n \\nBoosting \\nBoosting is an iterative technique which adjusts the weight of an observation based on the last  \\nclassification. If an observation was classified incorrectly, it tries to increase the weight of this  \\nobservation and vice versa. Boosting in general decreases the bias error and builds strong predictive  \\nmodels. \\n \\nVariance \\nVariance quantifies how the predictions made on same observation are different from each other. A  \\nhigh variance model will over -fit on your training population and perform badly on any observation  \\nbeyond training. Thus, we aim at low variance.'),\n",
              " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-04-04T07:17:46+05:30', 'author': 'Abhinav', 'moddate': '2024-04-04T07:17:46+05:30', 'source': '/content/drive/MyDrive/ens_d2.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='2  \\n \\nBias \\nBias error is useful to quantify how much on on average are the predicted values different from the  \\nactual value. A high  bias  error means we have a under -performing model.  Thus,  we aim at low  \\nbias. \\nA commonly used class of ensemble methods are forests of randomized trees. \\nIn random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e. \\na bootstrap sample) from the training set. In addition, instead of using all the features, a random  \\nsubset of features is selected, further randomizing the tree. \\nAs a result, the bias of the forest increases slightly, but due to the averaging of less correlated  \\ntrees, its variance decreases, resulting in an overall better model.')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0A3xSoDADmAf",
        "outputId": "5f7b495f-6534-4859-fe96-b5720d93b9cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \n",
            " \n",
            " \n",
            "N \n",
            " \n",
            "1 Principal Component Analysis \n",
            "In real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the  \n",
            "data and find various patterns in it or use it to train some machine learning models.  One way to  \n",
            "think about dimensions is that suppose you have an data point x , if we consider this data point as \n",
            "a physical object then dimensions are merely a basis of view, like where is the data located when \n",
            "it is observed from horizontal axis or vertical axis. \n",
            "As the dimensions of data increases, the difficulty to visualize it and perform computations on \n",
            "it also increases. So, how to reduce the dimensions of a data:- \n",
            "• Remove the redundant dimensions \n",
            "• Only keep the most important dimensions  \n",
            "Let us first try to understand some terms:- \n",
            "Variance : It is a measure of the variability or it simply measures how spread the data set is.  \n",
            "Mathematically, it is the average squared deviation from the mean score. We use the following \n",
            "formula to compute variance var(x). \n",
            " \n",
            "var(x) = \n",
            "Σ(xi−x¯)2 \n",
            "N \n",
            "Covariance :  It is a measure of the extent to which corresponding elements from two sets of  \n",
            "ordered data move in the same direction. Formula is shown below denoted by cov(x,y) as the  \n",
            "covariance of x and y. \n",
            "var(x)  =  \n",
            "Σ(xi−x¯)(yi−y¯) \n",
            "• Here, xi is the value of x in ith dimension. \n",
            "• x bar and y bar denote the corresponding mean values. \n",
            "• One way to observe the covariance is how interrelated two data sets are. \n",
            "Positive covariance means X and Y are positively related i.e. as X increases Y also increases.  \n",
            "Negative covariance depicts the exact opposite relation.  However zero covariance means X and Y  \n",
            "are not related. \n",
            "Now lets think about the requirement of data analysis. \n",
            "Since we try to find the patterns among the data sets so we want the data to be spread out  \n",
            "across each dimension. Also, we want the dimensions to be independent. Such that if data has high \n",
            "covariance when represented in some n number of dimensions then we replace those dimensions  \n",
            "with linear combination of those n dimensions. Now that data will only be dependent on linear  \n",
            "combination of those related n dimensions. (related = have high covariance)\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdrvHT_28jvw"
      },
      "source": [
        "### **Splitting of document**\n",
        "\n",
        "[Recursively split by character](https://api.python.langchain.com/en/latest/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)\n",
        "\n",
        "[Split by character](https://api.python.langchain.com/en/latest/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "duFVDKbk8qpo"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uKSoLugt9hgs"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4mjs7XDl9m6b",
        "outputId": "e931c008-5a74-4bdd-be7c-5b157aae93ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n",
            "498\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1 \\n \\n \\nN \\n \\n1 Principal Component Analysis \\nIn real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the  \\ndata and find various patterns in it or use it to train some machine learning models.  One way to  \\nthink about dimensions is that suppose you have an data point x , if we consider this data point as \\na physical object then dimensions are merely a basis of view, like where is the data located when \\nit is observed from horizontal axis or vertical axis.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(len(splits))\n",
        "print(len(splits[0].page_content) )\n",
        "splits[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "y-QuwWdH2-vt",
        "outputId": "10121730-a4df-4f50-dfae-fed52c2d07c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-04-04T07:17:15+05:30', 'author': 'Ramendra Kumar', 'moddate': '2024-04-04T07:17:15+05:30', 'source': '/content/drive/MyDrive/pca_d1.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='1 \\n \\n \\nN \\n \\n1 Principal Component Analysis \\nIn real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the  \\ndata and find various patterns in it or use it to train some machine learning models.  One way to  \\nthink about dimensions is that suppose you have an data point x , if we consider this data point as \\na physical object then dimensions are merely a basis of view, like where is the data located when \\nit is observed from horizontal axis or vertical axis.')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "splits[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuSgQ0p_-d97"
      },
      "source": [
        "### **Embeddings**\n",
        "\n",
        "Let's take our splits and embed them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rEcodF14Gkuv"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embedding = OpenAIEmbeddings(model='text-embedding-3-small')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OaXEK1AkGxod",
        "outputId": "585b7513-93c9-4ee4-a97e-8d05d1576033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x7fa7efdf5940>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x7fa7ef89a570>, model='text-embedding-3-small', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkTrtTYh_N1L"
      },
      "source": [
        "### **Understanding similarity search with a toy example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4L8_BLc6_O6Q"
      },
      "outputs": [],
      "source": [
        "sentence1 = \"i like dogs\"\n",
        "sentence2 = \"i like cats\"\n",
        "sentence3 = \"the weather is ugly, too hot outside\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "V8IZ6a4G_SzI"
      },
      "outputs": [],
      "source": [
        "embedding1 = embedding.embed_query(sentence1)\n",
        "embedding2 = embedding.embed_query(sentence2)\n",
        "embedding3 = embedding.embed_query(sentence3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_PkO3fMe_WOx",
        "outputId": "d22e06e3-319d-4f5c-b95e-25e332334b19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1536, 1536, 1536)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "len(embedding1), len(embedding2), len(embedding3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yROw-cqp32tk",
        "outputId": "2cfb758f-ea8e-4684-fc55-424e57e44e69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.01652492582798004,\n",
              " -0.033298008143901825,\n",
              " 5.593090918409871e-06,\n",
              " 0.006350534502416849,\n",
              " 0.027861136943101883,\n",
              " -0.011934041976928711,\n",
              " -0.007687192410230637,\n",
              " 0.0372685007750988,\n",
              " -0.07273223251104355,\n",
              " -0.022074593231081963]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "embedding1[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "of5PTiI6_v_7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vector1, vector2):\n",
        "    # Ensure that the vectors are numpy arrays\n",
        "    vector1 = np.array(vector1)\n",
        "    vector2 = np.array(vector2)\n",
        "\n",
        "    # Calculate the dot product of the vectors\n",
        "    dot_product = np.dot(vector1, vector2)\n",
        "\n",
        "    # Calculate the magnitude (norm) of the vectors\n",
        "    norm_vector1 = np.linalg.norm(vector1)\n",
        "    norm_vector2 = np.linalg.norm(vector2)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    if norm_vector1 == 0 or norm_vector2 == 0:\n",
        "        return 0  # Avoid division by zero\n",
        "    return dot_product / (norm_vector1 * norm_vector2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "m9aJr_JIAI4e",
        "outputId": "9859f411-e451-4038-dab9-8ab91ed28529",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(0.7222047452671774),\n",
              " np.float64(0.2025683886169236),\n",
              " np.float64(0.18210909214104934))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "cosine_similarity(embedding1, embedding2), cosine_similarity(embedding1, embedding3), cosine_similarity(embedding2, embedding3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c6sV1LZAn2L"
      },
      "source": [
        "### **Vectorstores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "P_-M_ifhAsAj"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma       # Light-weight and in memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "psySqG1WAwCZ"
      },
      "outputs": [],
      "source": [
        "persist_directory = 'docs/chroma/'\n",
        "!rm -rf ./docs/chroma  # remove old database files if any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Rfidz1UDA8B6"
      },
      "outputs": [],
      "source": [
        "vectordb = Chroma.from_documents(\n",
        "    documents=splits,                    # splits we created earlier\n",
        "    embedding=embedding,\n",
        "    persist_directory=persist_directory, # save the directory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "zy88zfxWBjOv",
        "outputId": "1a688082-7b20-4cdf-c8c7-1ab6fc33a82f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ],
      "source": [
        "print(vectordb._collection.count()) # same as number of splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34sLEBpRGm-u"
      },
      "source": [
        "### **Similarity Search in Vector store**\n",
        "\n",
        "Algorithms for retrieving relevant chunks In Vector databases,\n",
        "\n",
        "In vector databases, algorithms for retrieving relevant chunks to a query are often based on **similarity search techniques**, primarily using nearest neighbor search.\n",
        "\n",
        "Here are some common approaches:\n",
        "\n",
        "> **Approximate Nearest Neighbor (ANN) Search:** Vector databases frequently use ANN algorithms to improve efficiency when searching for vectors that\n",
        "are close to the query vector.\n",
        ">\n",
        "> Popular **ANN** algorithms include:\n",
        "\n",
        "> 1. HNSW (Hierarchical Navigable Small World Graph): This is a graph-based approach that finds approximate nearest neighbors using a multi-\n",
        "layered graph structure.\n",
        "\n",
        "> 2. Faiss: An open-source library developed by Facebook, which uses various algorithms for fast similarity search, such as Product Quantization and\n",
        "Inverted File System (IVF).\n",
        "\n",
        "> 3. Annoy (Approximate Nearest Neighbors Oh Yeah): Developed by Spotify, it uses a forest of random projection trees for approximate nearest\n",
        "neighbor search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "X0o4yUxfGo_W"
      },
      "outputs": [],
      "source": [
        "question = \"How does ensemble method works?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "collapsed": true,
        "id": "BZsCVQUrGsEm"
      },
      "outputs": [],
      "source": [
        "docs = vectordb.similarity_search(question, k=6)     # k --> No. of Document object to return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "XMvtnoEiaeqL",
        "outputId": "ab228a9c-6937-469c-be62-79a9ab708472",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "Why use Ensemble Methods? \n",
            "Ensemble Methods are used in order to: \n",
            "• decrease variance (bagging) \n",
            "• decrease bias (boosting) \n",
            "• improve predictions (stacking) \n",
            " \n",
            "Bagging \n",
            "Bagging actually refers to Bootstrap Aggregators. \n",
            "Bagging tests multiple models on the data by sampling and replacing data i.e it utilizes bootstrap - \n",
            "ping. In turn, this reduces the noise and variance by utilizing multiple samples. Each hypothesis\n",
            "============================================================================================================================================\n",
            "Why use Ensemble Methods? \n",
            "Ensemble Methods are used in order to: \n",
            "• decrease variance (bagging) \n",
            "• decrease bias (boosting) \n",
            "• improve predictions (stacking) \n",
            " \n",
            "Bagging \n",
            "Bagging actually refers to Bootstrap Aggregators. \n",
            "Bagging tests multiple models on the data by sampling and replacing data i.e it utilizes bootstrap - \n",
            "ping. In turn, this reduces the noise and variance by utilizing multiple samples. Each hypothesis\n",
            "============================================================================================================================================\n",
            "considered. The product is bought by the user when the combined ratings of the group is positive. \n",
            "The user gets a fairer idea about the product when all the ratings are combined. \n",
            "Here, the combination of ratings is done so that the decision making process of the user is made  \n",
            "easy. \n",
            "Ensemble Methods refer to combining many different machine learning models in order to get a  \n",
            "more powerful prediction. \n",
            "Thus, ensemble methods increase the accuracy of the predictions.\n",
            "============================================================================================================================================\n",
            "considered. The product is bought by the user when the combined ratings of the group is positive. \n",
            "The user gets a fairer idea about the product when all the ratings are combined. \n",
            "Here, the combination of ratings is done so that the decision making process of the user is made  \n",
            "easy. \n",
            "Ensemble Methods refer to combining many different machine learning models in order to get a  \n",
            "more powerful prediction. \n",
            "Thus, ensemble methods increase the accuracy of the predictions.\n",
            "============================================================================================================================================\n",
            "1  \n",
            " \n",
            "Ensemble Methods \n",
            "Let us consider a real world situation which uses Ensemble Methods, which is, when a user wants \n",
            "to buy a new product. Many users who have already purchased that product will have given either  \n",
            "positive or negative ratings. If in the group, many users have given positive ratings, then the \n",
            "combined rating will be positive. Instead of a single rating, the ratings of the group of users is\n",
            "============================================================================================================================================\n",
            "1  \n",
            " \n",
            "Ensemble Methods \n",
            "Let us consider a real world situation which uses Ensemble Methods, which is, when a user wants \n",
            "to buy a new product. Many users who have already purchased that product will have given either  \n",
            "positive or negative ratings. If in the group, many users have given positive ratings, then the \n",
            "combined rating will be positive. Instead of a single rating, the ratings of the group of users is\n",
            "============================================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(len(docs))\n",
        "\n",
        "for i in range(len(docs)):\n",
        "    print(docs[i].page_content)\n",
        "    print('='*140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F5AXqC6I3gN"
      },
      "source": [
        "### **Edge cases where failure may happen**\n",
        "\n",
        "1. Lack of Diversity : Semantic search fetches all similar documents, but does not enforce diversity.\n",
        "\n",
        "    - Notice that we're getting duplicate chunks (because of the duplicate `ens_d2.pdf` in the index). `docs[0]` and `docs[1]` are indentical.\n",
        "\n",
        "  **Addressing Diversity - MMR (Maximum Marginal Relevance)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE6AOc3ZTL8x"
      },
      "source": [
        "Maximum Marginal Relevance (MMR) is a method used to retrieve relevant items to a query while avoiding redundancy. It does this by ensuring a balance between relevancy and diversity in the items retrieved.\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:828/format:webp/1*U-9mPt5tBfPBPrwC4_oD1w.png'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "jpwufsTuI-Uz",
        "outputId": "fe911a3a-2583-459b-a689-d9b45934f1bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "Why use Ensemble Methods? \n",
            "Ensemble Methods are used in order to: \n",
            "• decrease variance (bagging) \n",
            "• decrease bias (boosting) \n",
            "• improve predictions (stacking) \n",
            " \n",
            "Bagging \n",
            "Bagging actually refers to Bootstrap Aggregators. \n",
            "Bagging tests multiple models on the data by sampling and replacing data i.e it utilizes bootstrap - \n",
            "ping. In turn, this reduces the noise and variance by utilizing multiple samples. Each hypothesis\n",
            "============================================================================================================================================\n",
            "Why use Ensemble Methods? \n",
            "Ensemble Methods are used in order to: \n",
            "• decrease variance (bagging) \n",
            "• decrease bias (boosting) \n",
            "• improve predictions (stacking) \n",
            " \n",
            "Bagging \n",
            "Bagging actually refers to Bootstrap Aggregators. \n",
            "Bagging tests multiple models on the data by sampling and replacing data i.e it utilizes bootstrap - \n",
            "ping. In turn, this reduces the noise and variance by utilizing multiple samples. Each hypothesis\n",
            "============================================================================================================================================\n",
            "considered. The product is bought by the user when the combined ratings of the group is positive. \n",
            "The user gets a fairer idea about the product when all the ratings are combined. \n",
            "Here, the combination of ratings is done so that the decision making process of the user is made  \n",
            "easy. \n",
            "Ensemble Methods refer to combining many different machine learning models in order to get a  \n",
            "more powerful prediction. \n",
            "Thus, ensemble methods increase the accuracy of the predictions.\n",
            "============================================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "question = 'How ensemble method works?'\n",
        "docs = vectordb.similarity_search(question, k=3)     # Without MMR\n",
        "\n",
        "print(len(docs))\n",
        "\n",
        "for i in range(len(docs)):\n",
        "    print(docs[i].page_content)\n",
        "    print('='*140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVlwfHLOJNET"
      },
      "source": [
        "**Example 1. Addressing Diversity - MMR-Maximum Marginal Relevance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "97gx2ZzBJO-z",
        "outputId": "68d71d4a-2ecf-4e86-8f4f-a2d26b82a41b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "Why use Ensemble Methods? \n",
            "Ensemble Methods are used in order to: \n",
            "• decrease variance (bagging) \n",
            "• decrease bias (boosting) \n",
            "• improve predictions (stacking) \n",
            " \n",
            "Bagging \n",
            "Bagging actually refers to Bootstrap Aggregators. \n",
            "Bagging tests multiple models on the data by sampling and replacing data i.e it utilizes bootstrap - \n",
            "ping. In turn, this reduces the noise and variance by utilizing multiple samples. Each hypothesis\n",
            "============================================================================================================================================\n",
            "considered. The product is bought by the user when the combined ratings of the group is positive. \n",
            "The user gets a fairer idea about the product when all the ratings are combined. \n",
            "Here, the combination of ratings is done so that the decision making process of the user is made  \n",
            "easy. \n",
            "Ensemble Methods refer to combining many different machine learning models in order to get a  \n",
            "more powerful prediction. \n",
            "Thus, ensemble methods increase the accuracy of the predictions.\n",
            "============================================================================================================================================\n",
            "1  \n",
            " \n",
            "Ensemble Methods \n",
            "Let us consider a real world situation which uses Ensemble Methods, which is, when a user wants \n",
            "to buy a new product. Many users who have already purchased that product will have given either  \n",
            "positive or negative ratings. If in the group, many users have given positive ratings, then the \n",
            "combined rating will be positive. Instead of a single rating, the ratings of the group of users is\n",
            "============================================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "docs_with_mmr = vectordb.max_marginal_relevance_search(question, k=3, fetch_k=6)   # With MMR\n",
        "\n",
        "print(len(docs_with_mmr))\n",
        "\n",
        "for i in range(len(docs_with_mmr)):\n",
        "    print(docs_with_mmr[i].page_content)\n",
        "    print('='*140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNmQcZc8nvDm"
      },
      "source": [
        "2. Lack of specificity:  The question may be from a particular doc but answer may contain information from other doc.\n",
        "\n",
        "  **Addressing Specificity: Working with metadata - Manually**\n",
        "\n",
        "  **Working with metadata using self-query retriever - Automatically**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsZDx8-fJ8fE"
      },
      "source": [
        "**Example 2. Addressing Specificity: Working with metadata - Manually**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9IlD3OQhJ-Fb",
        "outputId": "2c350b07-aeb9-4c32-fc32-215673ced2e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'page': 0, 'source': '/content/drive/MyDrive/pca_d1.pdf'}\n",
            "{'page': 0, 'source': '/content/drive/MyDrive/ens_d2.pdf'}\n",
            "{'page': 0, 'source': '/content/drive/MyDrive/ens_d2.pdf'}\n",
            "{'page': 0, 'source': '/content/drive/MyDrive/pca_d1.pdf'}\n",
            "{'page': 1, 'source': '/content/drive/MyDrive/pca_d1.pdf'}\n"
          ]
        }
      ],
      "source": [
        "# Without metadata information\n",
        "question = \"What is variance?\"\n",
        "\n",
        "docs = vectordb.similarity_search(question, k=5)\n",
        "\n",
        "for doc in docs:\n",
        "    print({'page': doc.metadata['page'], 'source': doc.metadata['source']})    # metadata contains information about from which doc the answer has been fetched"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCRPVfiSO_eC"
      },
      "source": [
        "We can filter the results based on metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "it6xqSSrJ-NT"
      },
      "outputs": [],
      "source": [
        "# With metadata information\n",
        "question = \"what is the role of variance in pca?\"\n",
        "docs = vectordb.similarity_search(\n",
        "    question,\n",
        "    k=5,\n",
        "    filter={\"source\":'/content/ens_d2.pdf'}     # manually passing metadata, using metadata filter.\n",
        ")\n",
        "\n",
        "for doc in docs:\n",
        "    print({'page': doc.metadata['page'], 'source': doc.metadata['source']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "B-6RjPCZR6IZ"
      },
      "outputs": [],
      "source": [
        "# With metadata information + MMR\n",
        "\n",
        "docs_with_mmr = vectordb.max_marginal_relevance_search(question,\n",
        "                                                       k=2,\n",
        "                                                       fetch_k=5,\n",
        "                                                       filter={\"source\":'/content/ens_d2.pdf'}     # manually passing metadata, using metadata filter.\n",
        "                                                       )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "BLF9imzhRJQJ"
      },
      "outputs": [],
      "source": [
        "for i in range(len(docs_with_mmr)):\n",
        "    print(docs_with_mmr[i].page_content)\n",
        "    print('='*140)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ad08RAEPhVN"
      },
      "source": [
        "[**Addressing Specificity -Automatically: Working with metadata using self-query retriever**](https://drive.google.com/file/d/1cwsZ19oCJFhQDEMfDmIWjEPOI-Fs-9iB/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSpvKgQDQW6a"
      },
      "source": [
        "### **Additional tricks: Compression**\n",
        "\n",
        "Another approach for improving the quality of retrieved docs is compression. Information most relevant to a query may be buried in a document with a lot of irrelevant text. Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
        "\n",
        "[Contextual compression](https://blog.langchain.com/improving-document-retrieval-with-contextual-compression/) is meant to fix this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6UERPlnQsoP"
      },
      "source": [
        "## **Retrieval**\n",
        "\n",
        "**Vectorstore as a retriever**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMc9GmSCT1on"
      },
      "source": [
        "**Better Approach**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "s84yVVx2Tgl-",
        "outputId": "6a712cb8-6fb2-4e4b-c620-3be178584c88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='e3b88648-f845-450c-866f-9f94d3fa3dda', metadata={'author': 'Ramendra Kumar', 'page_label': '2', 'source': '/content/drive/MyDrive/pca_d1.pdf', 'moddate': '2024-04-04T07:17:15+05:30', 'page': 1, 'total_pages': 3, 'creationdate': '2024-04-04T07:17:15+05:30', 'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021'}, page_content='2 \\n \\n \\n \\nSo, what does Principal Component Analysis (PCA) do? \\nPCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are  \\northogonal (and hence linearly independent) and ranked according to the variance of data along  \\nthem. It means more important principle axis occurs first. (more important = more variance/more  \\nspread out data) \\n \\nHow does PCA work? \\n• Calculate the covariance matrix X of data points.'),\n",
              " Document(id='27bbfb8e-9b06-4615-b3d7-e5acd5eb9035', metadata={'total_pages': 3, 'page_label': '1', 'producer': 'Microsoft® Word 2021', 'source': '/content/drive/MyDrive/pca_d1.pdf', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-04-04T07:17:15+05:30', 'author': 'Ramendra Kumar', 'moddate': '2024-04-04T07:17:15+05:30', 'page': 0}, page_content='1 \\n \\n \\nN \\n \\n1 Principal Component Analysis \\nIn real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the  \\ndata and find various patterns in it or use it to train some machine learning models.  One way to  \\nthink about dimensions is that suppose you have an data point x , if we consider this data point as \\na physical object then dimensions are merely a basis of view, like where is the data located when \\nit is observed from horizontal axis or vertical axis.'),\n",
              " Document(id='ec8fc7f8-5a51-43aa-ad9a-ed8c05078203', metadata={'source': '/content/drive/MyDrive/pca_d1.pdf', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-04-04T07:17:15+05:30', 'page': 1, 'moddate': '2024-04-04T07:17:15+05:30', 'producer': 'Microsoft® Word 2021', 'author': 'Ramendra Kumar', 'page_label': '2', 'total_pages': 3}, page_content='• Calculate eigenvectors and corresponding eigenvalues. \\n• Sort the eigenvectors according to their eigenvalues in decreasing order. \\n• Choose first k eigenvectors and that will be the new k dimensions. \\n• Transform the original n dimensional data points into k dimensions. \\n \\nTo understand the detail working of PCA, we should have knowledge of eigen values and eigen  \\nvectors \\n \\nEigenvectors: The directions in which our data are dispersed.')]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# Without MMR\n",
        "question = \"What is principal component analysis?\"\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
        "docs = retriever.invoke(question)\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "bfF5Yu7zT0QY",
        "outputId": "e5347438-6848-4076-eba5-d21a5bf360b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='e3b88648-f845-450c-866f-9f94d3fa3dda', metadata={'page': 1, 'creationdate': '2024-04-04T07:17:15+05:30', 'creator': 'Microsoft® Word 2021', 'total_pages': 3, 'producer': 'Microsoft® Word 2021', 'page_label': '2', 'author': 'Ramendra Kumar', 'moddate': '2024-04-04T07:17:15+05:30', 'source': '/content/drive/MyDrive/pca_d1.pdf'}, page_content='2 \\n \\n \\n \\nSo, what does Principal Component Analysis (PCA) do? \\nPCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are  \\northogonal (and hence linearly independent) and ranked according to the variance of data along  \\nthem. It means more important principle axis occurs first. (more important = more variance/more  \\nspread out data) \\n \\nHow does PCA work? \\n• Calculate the covariance matrix X of data points.'),\n",
              " Document(id='27bbfb8e-9b06-4615-b3d7-e5acd5eb9035', metadata={'page_label': '1', 'author': 'Ramendra Kumar', 'moddate': '2024-04-04T07:17:15+05:30', 'source': '/content/drive/MyDrive/pca_d1.pdf', 'page': 0, 'creationdate': '2024-04-04T07:17:15+05:30', 'producer': 'Microsoft® Word 2021', 'total_pages': 3, 'creator': 'Microsoft® Word 2021'}, page_content='1 \\n \\n \\nN \\n \\n1 Principal Component Analysis \\nIn real world data analysis tasks we analyze complex data i.e. multi dimensional data. We plot the  \\ndata and find various patterns in it or use it to train some machine learning models.  One way to  \\nthink about dimensions is that suppose you have an data point x , if we consider this data point as \\na physical object then dimensions are merely a basis of view, like where is the data located when \\nit is observed from horizontal axis or vertical axis.')]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# With MMR\n",
        "retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\":5})\n",
        "docs = retriever.invoke(question)\n",
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DyRvpuRQ8_T"
      },
      "source": [
        "## **Augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "86wr6Mv6WLX3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate                                    # To format prompts\n",
        "from langchain_core.output_parsers import StrOutputParser                            # to transform the output of an LLM into a more usable format\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough          # Required by LCEL (LangChain Expression Language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Ng1noLv0WVmY"
      },
      "outputs": [],
      "source": [
        "# Build prompt\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Always say \"thanks for asking!\" at the end of the answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "QA_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSIy0CnwMv2h"
      },
      "source": [
        "## **Creating final RAG Chain**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lELTaT0PwPG"
      },
      "source": [
        "> <img src='https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F63f8a8482c9ec06a8d7d1041514f87c06dd108a9-3442x942.png&w=3840&q=75' width=1200px>\n",
        "\n",
        "[[Image source](https://www.pinecone.io/learn/series/langchain/langchain-expression-language/)]\n",
        "\n",
        "Above figure describes the LCEL flow using `RunnableParallel` and `RunnablePassthrough`.\n",
        "\n",
        "A Runnable is a **unit of execution** in the LangChain framework. It represents a specific task or operation that can be performed.\n",
        "\n",
        "Examples of Runnables include data transformations, computations, or any other operation that can be **expressed** in the LCEL(LangChain expression language).\n",
        "\n",
        "[Runnable Lambdas](https://api.python.langchain.com/en/latest/core/runnables/langchain_core.runnables.base.RunnableLambda.html) is a LangChain abstraction that allows us to turn Python functions into **pipe-compatible functions**, similar to the Runnable class.\n",
        "\n",
        "[RunnablePassthrough](https://api.python.langchain.com/en/latest/core/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html) on its own allows you to pass inputs unchanged. This typically is **used in conjuction with [RunnableParallel](https://api.python.langchain.com/en/latest/core/runnables/langchain_core.runnables.base.RunnableParallel.html)** to pass data through to a new key in the map.\n",
        "\n",
        "The **RunnableParallel** object allows us to define multiple values and operations, and run them all in parallel.\n",
        "\n",
        "The **RunnablePassthrough** object is used as a “passthrough” that takes any input to the current component ('retrieval' in above figure) and allows us to provide it in the component output via the “question” key or any other custom key."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_context_info(question):\n",
        "    retriever = vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3, \"fetch_k\":5})\n",
        "    docs = retriever.invoke(question)\n",
        "    return docs\n"
      ],
      "metadata": {
        "id": "6_7JZ9lZ44Ly"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "retrieval = RunnableParallel(\n",
        "    {\n",
        "        \"context\": RunnableLambda(lambda x: get_context_info(x[\"question\"])),\n",
        "        \"question\": RunnableLambda(lambda x: x[\"question\"])\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "toDa-vnj2QAA"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "pprint(retrieval.invoke({\"question\": \"What is PCA ?\"}))"
      ],
      "metadata": {
        "id": "qSeaHqdh23H_",
        "outputId": "5ebc0024-34b6-4455-d72d-7ec2f38b9f6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'context': [Document(id='e3b88648-f845-450c-866f-9f94d3fa3dda', metadata={'page_label': '2', 'creationdate': '2024-04-04T07:17:15+05:30', 'page': 1, 'producer': 'Microsoft® Word 2021', 'moddate': '2024-04-04T07:17:15+05:30', 'creator': 'Microsoft® Word 2021', 'author': 'Ramendra Kumar', 'source': '/content/drive/MyDrive/pca_d1.pdf', 'total_pages': 3}, page_content='2 \\n \\n \\n \\nSo, what does Principal Component Analysis (PCA) do? \\nPCA finds a new set of dimensions (or a set of basis of views) such that all the dimensions are  \\northogonal (and hence linearly independent) and ranked according to the variance of data along  \\nthem. It means more important principle axis occurs first. (more important = more variance/more  \\nspread out data) \\n \\nHow does PCA work? \\n• Calculate the covariance matrix X of data points.'),\n",
            "             Document(id='ec8fc7f8-5a51-43aa-ad9a-ed8c05078203', metadata={'author': 'Ramendra Kumar', 'producer': 'Microsoft® Word 2021', 'page': 1, 'moddate': '2024-04-04T07:17:15+05:30', 'page_label': '2', 'creator': 'Microsoft® Word 2021', 'total_pages': 3, 'creationdate': '2024-04-04T07:17:15+05:30', 'source': '/content/drive/MyDrive/pca_d1.pdf'}, page_content='• Calculate eigenvectors and corresponding eigenvalues. \\n• Sort the eigenvectors according to their eigenvalues in decreasing order. \\n• Choose first k eigenvectors and that will be the new k dimensions. \\n• Transform the original n dimensional data points into k dimensions. \\n \\nTo understand the detail working of PCA, we should have knowledge of eigen values and eigen  \\nvectors \\n \\nEigenvectors: The directions in which our data are dispersed.'),\n",
            "             Document(id='848bab02-49a3-4e95-96b7-0b17a07d08b2', metadata={'creationdate': '2024-04-04T07:17:15+05:30', 'author': 'Ramendra Kumar', 'creator': 'Microsoft® Word 2021', 'total_pages': 3, 'page': 2, 'producer': 'Microsoft® Word 2021', 'moddate': '2024-04-04T07:17:15+05:30', 'page_label': '3', 'source': '/content/drive/MyDrive/pca_d1.pdf'}, page_content='covariance is a diagonal matrix. \\nAlways normalize your data before doing PCA if we use data (features here) of different scales, we \\nget misleading components. We can also simply use correlation matrix instead of using covariance  \\nmatrix if features are of different scales. \\nThis defines the goal of PCA:- \\n1. Find linearly independent dimensions which can losslessly represent the data points. \\n2. Those newly found dimensions should allow us to predict/reconstruct the original dimensions.')],\n",
            " 'question': 'What is PCA ?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(retrieval.invoke({\"question\": \"How ensemble methods works?\"}))"
      ],
      "metadata": {
        "id": "lmKEiPxk4IVA",
        "outputId": "3e3084c6-0380-4835-b43f-fc1235bcf539",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'context': [Document(id='efb09d85-eb32-456d-985d-af39c669a1b9', metadata={'creator': 'Microsoft® Word 2021', 'page_label': '1', 'source': '/content/drive/MyDrive/ens_d2.pdf', 'producer': 'Microsoft® Word 2021', 'page': 0, 'author': 'Abhinav', 'moddate': '2024-04-04T07:17:46+05:30', 'creationdate': '2024-04-04T07:17:46+05:30', 'total_pages': 2}, page_content='Why use Ensemble Methods? \\nEnsemble Methods are used in order to: \\n• decrease variance (bagging) \\n• decrease bias (boosting) \\n• improve predictions (stacking) \\n \\nBagging \\nBagging actually refers to Bootstrap Aggregators. \\nBagging tests multiple models on the data by sampling and replacing data i.e it utilizes bootstrap - \\nping. In turn, this reduces the noise and variance by utilizing multiple samples. Each hypothesis'),\n",
            "             Document(id='c44dbcbe-5c0b-40a4-9a90-b5fcc31467d9', metadata={'page_label': '1', 'creationdate': '2024-04-04T07:17:46+05:30', 'total_pages': 2, 'author': 'Abhinav', 'producer': 'Microsoft® Word 2021', 'moddate': '2024-04-04T07:17:46+05:30', 'source': '/content/drive/MyDrive/ens_d2.pdf', 'page': 0, 'creator': 'Microsoft® Word 2021'}, page_content='considered. The product is bought by the user when the combined ratings of the group is positive. \\nThe user gets a fairer idea about the product when all the ratings are combined. \\nHere, the combination of ratings is done so that the decision making process of the user is made  \\neasy. \\nEnsemble Methods refer to combining many different machine learning models in order to get a  \\nmore powerful prediction. \\nThus, ensemble methods increase the accuracy of the predictions.'),\n",
            "             Document(id='a0f8983e-6a1c-40ef-b22e-0ba560164ab7', metadata={'creator': 'Microsoft® Word 2021', 'producer': 'Microsoft® Word 2021', 'source': '/content/drive/MyDrive/ens_d2.pdf', 'page_label': '1', 'moddate': '2024-04-04T07:17:46+05:30', 'page': 0, 'creationdate': '2024-04-04T07:17:46+05:30', 'total_pages': 2, 'author': 'Abhinav'}, page_content='1  \\n \\nEnsemble Methods \\nLet us consider a real world situation which uses Ensemble Methods, which is, when a user wants \\nto buy a new product. Many users who have already purchased that product will have given either  \\npositive or negative ratings. If in the group, many users have given positive ratings, then the \\ncombined rating will be positive. Instead of a single rating, the ratings of the group of users is')],\n",
            " 'question': 'How ensemble methods works?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ACe-oEU3g5Mq"
      },
      "outputs": [],
      "source": [
        "# RAG Chain\n",
        "\n",
        "rag_chain = (retrieval                     # Retrieval\n",
        "             | QA_PROMPT                   # Augmentation\n",
        "             | llm                         # Generation\n",
        "             | StrOutputParser()\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "h5njKoSDUsAT",
        "outputId": "55e727f0-4551-4b23-d4ab-a49eb779cf76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of data while preserving as much variance as possible. It achieves this by finding a new set of dimensions (or basis) that are orthogonal (linearly independent) and ranked according to the variance of the data along them. The process involves calculating the covariance matrix of the data points, determining the eigenvectors and eigenvalues, sorting the eigenvectors by their eigenvalues in decreasing order, and selecting the top k eigenvectors to form the new dimensions. PCA is particularly useful for simplifying data, visualizing it, and improving the performance of machine learning algorithms.\\n\\nThanks for asking!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "response = rag_chain.invoke({\"question\": \"What is PCA ?\"})\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "tzgSs_H_U0YD",
        "outputId": "de2c5e45-cb8e-41b9-9874-c20447f2d0dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Principal Component Analysis (PCA) is a statistical technique used in data analysis to reduce the dimensionality of complex, multi-dimensional data while preserving as much variance as possible. It achieves this by finding a new set of dimensions (or basis of views) that are orthogonal (linearly independent) and ranked according to the variance of the data along them. The principal axes that capture the most variance are prioritized, allowing for a more efficient representation of the data. PCA is particularly useful for visualizing data, identifying patterns, and preparing data for machine learning models.\\n\\nThanks for asking!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "response = rag_chain.invoke({\"question\": \"What is principal component analysis?\"})\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "t5ju2XCJVB4v",
        "outputId": "54eb1dbc-277e-4a74-f287-2e55d090cb9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble methods work by combining multiple machine learning models to improve the overall prediction accuracy. There are different techniques within ensemble methods, including:\n",
            "\n",
            "1. **Bagging (Bootstrap Aggregating)**: This technique reduces variance by training multiple models on different subsets of the data, created through sampling with replacement. Each model makes predictions, and the final output is typically the average (for regression) or majority vote (for classification) of these predictions.\n",
            "\n",
            "2. **Boosting**: This method aims to reduce bias by sequentially training models, where each new model focuses on the errors made by the previous ones. The predictions are combined to create a stronger overall model.\n",
            "\n",
            "3. **Stacking**: In this approach, multiple models are trained, and their predictions are used as inputs to a higher-level model, which makes the final prediction. This can leverage the strengths of different models to improve accuracy.\n",
            "\n",
            "Overall, ensemble methods enhance predictions by leveraging the diversity of multiple models, leading to more robust and accurate outcomes. \n",
            "\n",
            "Thanks for asking!\n"
          ]
        }
      ],
      "source": [
        "response = rag_chain.invoke({\"question\": \"How ensemble method works?\"})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "TnpL5ouzWwd3",
        "outputId": "8c08fc2f-6739-4219-a3a0-1a62957fc4f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know. Thanks for asking!\n"
          ]
        }
      ],
      "source": [
        "# For queries that is not in documents\n",
        "response = rag_chain.invoke({\"question\": \"Who is the CEO of OpenAI \"})\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK8ISVozuENp"
      },
      "source": [
        "[**Details of Chroma through LangChain**](https://python.langchain.com/docs/integrations/vectorstores/chroma/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T97SnUYQuhIC"
      },
      "source": [
        "## Reusing Vector DB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7OnDFDbk_BI"
      },
      "source": [
        "### **Download the vector DB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "A0_0aghhlBzP",
        "outputId": "beb10e2c-3284-4913-b2e0-2d51e0bc1844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/docs/ (stored 0%)\n",
            "  adding: content/docs/chroma/ (stored 0%)\n",
            "  adding: content/docs/chroma/chroma.sqlite3 (deflated 58%)\n",
            "  adding: content/docs/chroma/1e70a146-ac3e-4014-b014-74466990cb73/ (stored 0%)\n",
            "  adding: content/docs/chroma/1e70a146-ac3e-4014-b014-74466990cb73/link_lists.bin (stored 0%)\n",
            "  adding: content/docs/chroma/1e70a146-ac3e-4014-b014-74466990cb73/length.bin (deflated 70%)\n",
            "  adding: content/docs/chroma/1e70a146-ac3e-4014-b014-74466990cb73/header.bin (deflated 63%)\n",
            "  adding: content/docs/chroma/1e70a146-ac3e-4014-b014-74466990cb73/data_level0.bin (deflated 100%)\n"
          ]
        }
      ],
      "source": [
        "# Zip the entire folder\n",
        "!zip -r /content/docs.zip /content/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "koEmhPTMlE5o",
        "outputId": "8491e05c-66a4-456b-aea4-368b46ae9f1c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ccd813a6-953f-44a7-83c0-29f73ee79e3c\", \"docs.zip\", 210174)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/docs.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7bjem50lIzp"
      },
      "source": [
        "### **Upload the vector db from previous step and unzip**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBND2d4glLY-",
        "outputId": "baf7544d-43ae-4abe-cfa9-18852ab36c10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/docs.zip\n",
            "replace /content/docs/chroma/chroma.sqlite3? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /content/docs/chroma/chroma.sqlite3  \n",
            "replace /content/docs/chroma/1e70a146-ac3e-4014-b014-74466990cb73/link_lists.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            " extracting: /content/docs/chroma/1e70a146-ac3e-4014-b014-74466990cb73/link_lists.bin  \n",
            "replace /content/docs/chroma/1e70a146-ac3e-4014-b014-74466990cb73/length.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /content/docs/chroma/1e70a146-ac3e-4014-b014-74466990cb73/length.bin  \n",
            "replace /content/docs/chroma/1e70a146-ac3e-4014-b014-74466990cb73/header.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /content/docs/chroma/1e70a146-ac3e-4014-b014-74466990cb73/header.bin  \n",
            "replace /content/docs/chroma/1e70a146-ac3e-4014-b014-74466990cb73/data_level0.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: /content/docs/chroma/1e70a146-ac3e-4014-b014-74466990cb73/data_level0.bin  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/docs.zip  -d /"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "n3Msb--vlQCy"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embedding = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "\n",
        "vectordb = Chroma(persist_directory = 'docs/chroma/',\n",
        "                  embedding_function = embedding\n",
        "                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m51wiPEp9T1"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "mVpko2ucp9UF"
      },
      "outputs": [],
      "source": [
        "#@title One key advantage of RAG over a standalone LLM is: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"Ability to use updated external knowledge\" #@param [\"\", \"Smaller model size\", \"Faster image generation\", \"Ability to use updated external knowledge\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "KjAEjFGssRpe"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "B1P8866ssRpe"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "2Tq8HTMpsRpe"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "tf-bFAj8sRpe"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "SgHtrsx0sRpe"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "cellView": "form",
        "id": "DTkh6ZUxsRpf",
        "outputId": "8877b2fc-53b4-4b8c-d6f7-56a9dcc4a3db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 2420\n",
            "Date of submission:  16 Feb 2026\n",
            "Time of submission:  20:41:16\n",
            "View your submissions: https://learn-iitm.talentsprint.com/notebook_submissions\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}